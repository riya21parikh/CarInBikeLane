# -*- coding: utf-8 -*-
"""SegFormer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15tYgfo-HVebKCW4QSPyrJecOJC1g4r1v

# Initial setup
"""

!git clone https://github.com/riya21parikh/CarInBikeLane.git

import os
import re
import platform
import time
import PIL
from PIL import Image
from tqdm import tqdm
from pathlib import Path
import json, glob

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

!pip install -q transformers accelerate
from sklearn.model_selection import train_test_split
import sklearn
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support
import tensorflow as tf
import keras
from keras import layers, models
from keras.preprocessing.image import load_img, img_to_array
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoImageProcessor, SegformerModel
import torchvision.transforms as T

import random
random.seed(42)

IMG_HEIGHT = 224
IMG_WIDTH = 224

"""# Data pre-processing

We start from a set of images split into two classes:

- `blocked`   – bike lane is fully obstructed (e.g., car parked in the lane)
- `notblocked` – bike lane is clear

Images from multiple sources/folders are:
1. Deduplicated and checked for corruption
2. Split into **train**, **validation**, and **test** sets each with blocked and notblocked components

The test set is intentionally built from a different camera (cam68) to simulate a real-world generalization scenario.
We also have a baseline_masks.json file containing manually annotated bike lane polygons for a subset of images. These masks identify the precise region of the lane in each image.

## Loading and concatenating + removing duplicates/corrupted images
"""

root = "/content/CarInBikeLane/Method1/data/data"
root2 = "/content/CarInBikeLane/Method1/data/data2"
root3 = "/content/CarInBikeLane/Method2/Bikelanes"

# Load all data
blocked2 = os.path.join(root, "blocked2")
blocked3 = os.path.join(root2, "blocked3")

notblocked2 = os.path.join(root, "notblocked2")
notblocked3 = os.path.join(root2, "notblocked3")

blocked = os.path.join(root3, "blocked")
notblocked = os.path.join(root3, "notblocked")

# concatenate them
blocked_folders = [
    os.path.join(root, "blocked2"),
    os.path.join(root2, "blocked3"),
    os.path.join(root3, "blocked"),
]

notblocked_folders = [
    os.path.join(root, "notblocked2"),
    os.path.join(root2, "notblocked3"),
    os.path.join(root3, "notblocked"),
]

blocked_files = []
notblocked_files = []

for folder in blocked_folders:
    blocked_files += [os.path.join(folder, f) for f in os.listdir(folder)]

for folder in notblocked_folders:
    notblocked_files += [os.path.join(folder, f) for f in os.listdir(folder)]

print("Total blocked images: ", len(blocked_files))
print("Total unblocked images: ", len(notblocked_files))

def remove_duplicates(file_list):
    seen = set()
    unique = []
    for f in file_list:
        name = os.path.basename(f)
        if name not in seen:
            seen.add(name)
            unique.append(f)
    return unique

def corrupted(path):
    try:
        img = PIL.Image.open(path)
        img.verify()
        return False
    except:
        return True

blocked_files = remove_duplicates(blocked_files)
notblocked_files = remove_duplicates(notblocked_files)

print("Total blocked images (no duplicates): ", len(blocked_files))
print("Total unblocked images (no duplicates): ", len(notblocked_files))

bad = []

for f in tqdm(blocked_files + notblocked_files):
    if corrupted(f):
        bad.append(f)

print("\n")
print("corrupted files:", len(bad))

"""## Data splitting (into folders to be used by models)

### Split by camera (test only on camera 68 and 70/30 split for train/val)
"""

import os, random, shutil

BASE = "/content/split_camera"

classes = ["blocked", "notblocked"]

if os.path.exists(BASE):
    shutil.rmtree(BASE)

for split in ["train", "val", "test"]:
    for cls in classes:
        os.makedirs(os.path.join(BASE, split, cls), exist_ok=True)

def is_cam68(path):
    return "cam68" in os.path.basename(path)


test_blocked      = [f for f in blocked_files if is_cam68(f)]
test_notblocked   = [f for f in notblocked_files if is_cam68(f)]

blocked_remaining     = [f for f in blocked_files if f not in test_blocked]
notblocked_remaining  = [f for f in notblocked_files if f not in test_notblocked]


def split_70_30(files):
    random.shuffle(files)
    n = len(files)
    n_train = int(0.7 * n)
    train = files[:n_train]
    val   = files[n_train:]
    return train, val


train_blocked, val_blocked = split_70_30(blocked_remaining)
train_notblocked, val_notblocked = split_70_30(notblocked_remaining)


for f in train_blocked:
    shutil.copy(f, os.path.join(BASE, "train/blocked"))
for f in train_notblocked:
    shutil.copy(f, os.path.join(BASE, "train/notblocked"))

for f in val_blocked:
    shutil.copy(f, os.path.join(BASE, "val/blocked"))
for f in val_notblocked:
    shutil.copy(f, os.path.join(BASE, "val/notblocked"))

for f in test_blocked:
    shutil.copy(f, os.path.join(BASE, "test/blocked"))
for f in test_notblocked:
    shutil.copy(f, os.path.join(BASE, "test/notblocked"))


print("DONE!")
print("Dataset created at:", BASE)
print()
print("blocked: train",len(train_blocked), "val",len(val_blocked), "test",len(test_blocked))
print("notblocked: train",len(train_notblocked), "val",len(val_notblocked), "test",len(test_notblocked))

"""# A/B Testing with Baseline Model"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# paths, labels, mask JSON
BASE = "/content/split_camera"

label_map = {
    "notblocked": 0,
    "blocked": 1
}
label_id2name = {v: k for k, v in label_map.items()}

# baseline_masks.json somewhere under /content
mask_meta = None
target_name = "baseline_masks.json"
found_path = None

for root, dirs, files in os.walk("/content"):
    if target_name in files:
        found_path = os.path.join(root, target_name)
        break

if found_path is not None:
    print("Found baseline_masks.json at:", found_path)
    with open(found_path, "r") as f:
        mask_meta = json.load(f)
    print("Loaded mask metadata for", len(mask_meta), "images")
else:
    print("baseline_masks.json NOT found under /content; lane cropping will only work if this file exists.")

# segformer processor & backbone
processor = AutoImageProcessor.from_pretrained(
    "nvidia/segformer-b0-finetuned-ade-512-512"
)

def make_backbone():
    backbone = SegformerModel.from_pretrained(
        "nvidia/segformer-b0-finetuned-ade-512-512"
    )
    backbone.to(device)
    return backbone

# data augmentation (applied to PIL image after optional crop)
# initially, what we had seen in the experiment before this was that the same image was being misclassified multiple times.
# since the cam angles dont really change and since pics are taken from footage around same time each day, there isn't a lot of variation in imgs
# hence it could be helpful to apply changes in the data/permutations like flips, rotations, color changing, crops/zooms
train_aug = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(degrees=10),
    T.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.2,
        hue=0.05
    ),
    # zoom-ish crop; SegFormer processor will still do final resize
    T.RandomResizedCrop(
        size=(512, 512),
        scale=(0.8, 1.0),
        ratio=(0.9, 1.1),
    ),
])

# dataset with optional lane cropping + augmentation
class BikeLaneFolderDataset(Dataset):
    def __init__(self, root_dir, processor, label_map,
                 mask_meta=None, use_lane_crop=False,
                 augment=False):
        self.root_dir = Path(root_dir)
        self.processor = processor
        self.label_map = label_map
        self.mask_meta = mask_meta
        self.use_lane_crop = use_lane_crop
        self.augment = augment

        self.samples = []
        for label_name in label_map.keys():
            img_dir = self.root_dir / label_name
            for ext in ("*.jpg", "*.jpeg", "*.png"):
                for img_path in img_dir.glob(ext):
                    self.samples.append((img_path, label_name))

        print(f"{root_dir}: found {len(self.samples)} images "
              f"(use_lane_crop={self.use_lane_crop}, augment={self.augment})")

    def _crop_to_lane(self, img, img_name):
        """
        Use baseline_masks.json to crop around the Bikelane polygon.
        If no mask/entry is found, returns the full image.
        """
        if self.mask_meta is None:
            return img

        key = img_name  # keys in JSON are filenames like "2016-09-16 151457 cam135.png"
        if key not in self.mask_meta:
            return img

        regions = self.mask_meta[key].get("regions", {})
        xs, ys = [], []

        for r in regions.values():
            sa = r.get("shape_attributes", {})
            if sa.get("name") == "polygon":
                xs.extend(sa.get("all_points_x", []))
                ys.extend(sa.get("all_points_y", []))

        if not xs or not ys:
            return img

        xmin, xmax = max(min(xs), 0), min(max(xs), img.width)
        ymin, ymax = max(min(ys), 0), min(max(ys), img.height)

        margin = 5  # small padding
        xmin = max(xmin - margin, 0)
        ymin = max(ymin - margin, 0)
        xmax = min(xmax + margin, img.width)
        ymax = min(ymax + margin, img.height)

        return img.crop((xmin, ymin, xmax, ymax))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label_name = self.samples[idx]
        img = Image.open(img_path).convert("RGB")

        # optional lane-based crop
        if self.use_lane_crop:
            img = self._crop_to_lane(img, img_path.name)

        # optional augmentation
        if self.augment:
            img = train_aug(img)

        # resize/normalize
        enc = self.processor(images=img, return_tensors="pt")
        pixel_values = enc["pixel_values"].squeeze(0)  # (3, H, W)

        label = self.label_map[label_name]
        return pixel_values, torch.tensor(label, dtype=torch.long)

# data loaders for if we do full-frame or lane-crop
def make_loaders(use_lane_crop, batch_size=8):
    train_ds = BikeLaneFolderDataset(
        os.path.join(BASE, "train"),
        processor,
        label_map,
        mask_meta=mask_meta,
        use_lane_crop=use_lane_crop,
        augment=True          # augment train, will turn this off for the second A/B
    )
    val_ds = BikeLaneFolderDataset(
        os.path.join(BASE, "val"),
        processor,
        label_map,
        mask_meta=mask_meta,
        use_lane_crop=use_lane_crop,
        augment=False         # no aug on val
    )
    test_ds = BikeLaneFolderDataset(
        os.path.join(BASE, "test"),
        processor,
        label_map,
        mask_meta=mask_meta,
        use_lane_crop=use_lane_crop,
        augment=False         # no aug on test
    )

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

# segformer classifier head
class SegFormerBlockedClassifier(nn.Module):
    def __init__(self, backbone, num_classes=2):
        super().__init__()
        self.backbone = backbone
        hidden_dim = backbone.config.hidden_sizes[-1]  # last encoder stage size
        self.classifier = nn.Linear(hidden_dim, num_classes)

    def forward(self, pixel_values):
        # pixel_values: (B, 3, H, W)
        outputs = self.backbone(pixel_values=pixel_values)
        feats = outputs.last_hidden_state  # (B, C, H', W')
        feats = feats.mean(dim=[2, 3])     # global average pooling -> (B, C)
        logits = self.classifier(feats)    # (B, num_classes)
        return logits

# evaluation helpers
def evaluate(model, loader):
    model.eval()
    all_labels = []
    all_preds  = []
    with torch.no_grad():
        for pixel_values, labels in loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            logits = model(pixel_values)
            preds = logits.argmax(dim=1)

            all_labels.extend(labels.cpu().tolist())
            all_preds.extend(preds.cpu().tolist())

    acc = accuracy_score(all_labels, all_preds)
    cm  = confusion_matrix(all_labels, all_preds, labels=[0, 1])
    report = classification_report(
        all_labels, all_preds, labels=[0, 1],
        target_names=["notblocked", "blocked"],
        digits=3
    )
    return acc, cm, report

# train model (baseline or lane-crop)
def train_one_model(name, use_lane_crop, num_epochs=8, lr=1e-4, batch_size=8):
    print(f"Training model: {name} (use_lane_crop={use_lane_crop})")

    train_loader, val_loader, test_loader = make_loaders(use_lane_crop, batch_size=batch_size)

    backbone = make_backbone()
    model = SegFormerBlockedClassifier(backbone, num_classes=2).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    train_losses = []
    val_accs = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for pixel_values, labels in train_loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            logits = model(pixel_values)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)

        train_loss = running_loss / len(train_loader.dataset)
        val_acc, _, _ = evaluate(model, val_loader)

        train_losses.append(train_loss)
        val_accs.append(val_acc)

        print(f"[{name}] Epoch {epoch+1}/{num_epochs} - "
              f"train_loss: {train_loss:.4f} - val_acc: {val_acc:.4f}")

    # plot loss + val accuracy curve to pick epoch #
    epochs = range(1, num_epochs + 1)

    plt.figure(figsize=(8,4))
    plt.plot(epochs, train_losses, marker="o")
    plt.title(f"{name} - Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(8,4))
    plt.plot(epochs, val_accs, marker="o")
    plt.title(f"{name} - Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.ylim(0, 1.0)
    plt.grid(True)
    plt.show()

    # final test evaluation
    test_acc, test_cm, test_report = evaluate(model, test_loader)
    print(f"\n[{name}] Test accuracy: {test_acc:.4f}")
    print(f"[{name}] Test confusion matrix (rows = true, cols = pred; 0=notblocked,1=blocked):")
    print(test_cm)
    print(f"\n[{name}] Test classification report:")
    print(test_report)

    history = {
        "train_losses": train_losses,
        "val_accs": val_accs
    }
    return model, (test_acc, test_cm, test_report, history)

"""We want to be able to tell how data augmentation affects the accuracy, but we have to make sure we run on both model types (full image and lane crop). For the full image model, we load the data, apply augmentation to training only, and then pass it through the segformer/classifier. For the lane crop model, we use the masks but also full images for images we don't have masks for; then we apply data augmentation and pass through the same way.

when augmentation = true, test baseline vs lane crop
"""

# a/b test - baseline vs lane-crop
baseline_model, baseline_metrics = train_one_model(
    name="baseline_full_frame",
    use_lane_crop=False,   # full image
    num_epochs=5,
    lr=1e-4,
    batch_size=8
)

lane_model, lane_metrics = train_one_model(
    name="lane_crop",
    use_lane_crop=True,    # crop using baseline_masks.json where available, else default to full
    num_epochs=5,
    lr=1e-4,
    batch_size=8
)

"""when augment = false, test baseline vs crop"""

# a/b test - baseline vs lane-crop
baseline_model, baseline_metrics = train_one_model(
    name="baseline_full_frame",
    use_lane_crop=False,   # full image
    num_epochs=5,
    lr=1e-4,
    batch_size=5
)

lane_model, lane_metrics = train_one_model(
    name="lane_crop",
    use_lane_crop=True,    # crop using baseline_masks.json where available, else default to full
    num_epochs=5,
    lr=1e-4,
    batch_size=5
)

"""inspect misclassified below"""

from pathlib import Path
import os
from PIL import Image
import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tqdm import tqdm
import matplotlib.pyplot as plt

label_id2name = {v: k for k, v in label_map.items()}

# test set, as seen by each model
baseline_test_ds = BikeLaneFolderDataset(
    os.path.join(BASE, "test"),
    processor,
    label_map,
    mask_meta=mask_meta,
    use_lane_crop=False
)

lane_test_ds = BikeLaneFolderDataset(
    os.path.join(BASE, "test"),
    processor,
    label_map,
    mask_meta=mask_meta,
    use_lane_crop=True
)

def collect_preds(model, dataset, name="model"):
    model.eval()
    results = []

    with torch.no_grad():
        for idx in tqdm(range(len(dataset)), desc=f"Running {name}"):
            img_path, label_name = dataset.samples[idx]  # original path + class name
            pixel_values, label_tensor = dataset[idx]

            pixel_values = pixel_values.unsqueeze(0).to(device)  # (1,3,H,W)
            label = label_tensor.item()

            logits = model(pixel_values)
            pred = logits.argmax(dim=1).item()

            results.append({
                "idx": idx,
                "path": str(img_path),
                "true_id": label,
                "true_name": label_id2name[label],
                "pred_id": pred,
                "pred_name": label_id2name[pred],
            })

    misclassified = [r for r in results if r["true_id"] != r["pred_id"]]
    print(f"{name}: {len(misclassified)}/{len(dataset)} misclassified")
    return results, misclassified

baseline_results, baseline_missed = collect_preds(
    baseline_model, baseline_test_ds, name="baseline_full_frame"
)

lane_results, lane_missed = collect_preds(
    lane_model, lane_test_ds, name="lane_crop"
)

def print_misclassified(mis_list, title, max_rows=None):
    print(title)
    for i, r in enumerate(mis_list):
        if max_rows is not None and i >= max_rows:
            break
        print(f"{i+1:2d}. {r['path']}")
        print(f"    true: {r['true_name']}   pred: {r['pred_name']}")
    print()

print_misclassified(baseline_missed, "Baseline full-frame misclassified")
print_misclassified(lane_missed,     "Lane-crop misclassified")

def show_misclassified(mis_list, title, n=6):
    n = min(n, len(mis_list))
    if n == 0:
        print(f"No misclassified examples for {title}")
        return

    cols = 3
    rows = (n + cols - 1) // cols

    plt.figure(figsize=(5*cols, 4*rows))
    for i in range(n):
        r = mis_list[i]
        img = Image.open(r["path"]).convert("RGB")

        plt.subplot(rows, cols, i+1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"true: {r['true_name']}\npred: {r['pred_name']}", fontsize=10)

    plt.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.show()

show_misclassified(baseline_missed, "Baseline full-frame – misclassified", n=8)
show_misclassified(lane_missed,     "Lane-crop – misclassified",          n=8)

"""From tests above, we can clearly see that augmentation aids our model. Since we have limited image diversity - images only come from 4 locations - it is important we add different sort of permutations to them to make our model think its seeing more new and diverse images. For the full frame accuracies, using augmentation resulted in +0.05. For the lane crop accuracies, the augmentation added +0.08. Overall, using the whole image rather than just the cropped portion was better w lower validation loss. We continue below.

# Best current model is that with augmentation and using the full image
"""

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
seed=42
set_seed(42)

SAVE_DIR = "/content/bike_lane_runs"
os.makedirs(SAVE_DIR, exist_ok=True)
print("Saving checkpoints/configs to:", SAVE_DIR)

# updated old function
def train_one_model(
    name,
    use_lane_crop,
    max_epochs=15,
    lr=1e-4,
    batch_size=8,
    save_dir=SAVE_DIR,
    seed=seed,
    min_delta=1e-3,       # min improvement in val_acc to count as better
    patience=3            # stop if no meaningful improvement after 3 epochs
):
    print(f"Training model: {name} (use_lane_crop={use_lane_crop})")

    set_seed(seed)

    train_loader, val_loader, test_loader = make_loaders(
        use_lane_crop, batch_size=batch_size
    )

    backbone = make_backbone()
    model = SegFormerBlockedClassifier(backbone, num_classes=2).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    train_losses = []
    val_accs = []

    best_val_acc = -1.0
    best_epoch = -1
    epochs_no_improve = 0

    os.makedirs(save_dir, exist_ok=True)
    best_ckpt_path = os.path.join(save_dir, f"{name}_best.pt")

    for epoch in range(1, max_epochs + 1):
        model.train()
        running_loss = 0.0

        for pixel_values, labels in train_loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            logits = model(pixel_values)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)

        train_loss = running_loss / len(train_loader.dataset)
        val_acc, _, _ = evaluate(model, val_loader)

        train_losses.append(train_loss)
        val_accs.append(val_acc)

        print(f"[{name}] Epoch {epoch}/{max_epochs} - "
              f"train_loss: {train_loss:.4f} - val_acc: {val_acc:.4f}")

        # check for improvement
        if val_acc > best_val_acc + min_delta:
            best_val_acc = val_acc
            best_epoch = epoch
            epochs_no_improve = 0

            torch.save(
                {
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "epoch": best_epoch,
                    "val_acc": best_val_acc,
                    "seed": seed,
                    "use_lane_crop": use_lane_crop,
                    "lr": lr,
                    "batch_size": batch_size,
                    "min_delta": min_delta,
                    "patience": patience,
                },
                best_ckpt_path,
            )
            print(f"  ↳ New best val_acc={best_val_acc:.4f} at epoch {best_epoch}, "
                  f"saved to {best_ckpt_path}")
        else:
            epochs_no_improve += 1
            print(f"  ↳ No significant improvement (Δval_acc={val_acc - best_val_acc:.4f}), "
                  f"epochs_no_improve={epochs_no_improve}/{patience}")

        # early stopping?
        if epochs_no_improve >= patience:
            print(f"\nEarly stopping triggered at epoch {epoch} "
                  f"(no improvement > {min_delta} for {patience} epochs).")
            break

    # plot
    epochs_ran = range(1, len(train_losses) + 1)

    plt.figure(figsize=(8, 4))
    plt.plot(epochs_ran, train_losses, marker="o")
    plt.title(f"{name} - Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(8, 4))
    plt.plot(epochs_ran, val_accs, marker="o")
    plt.title(f"{name} - Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.ylim(0, 1.0)
    plt.grid(True)
    plt.show()

    # best checkpoint
    print(f"\n[{name}] Loading best checkpoint from epoch {best_epoch} "
          f"(val_acc={best_val_acc:.4f})")
    checkpoint = torch.load(best_ckpt_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])

    # test eval
    test_acc, test_cm, test_report = evaluate(model, test_loader)
    print(f"\n[{name}] TEST accuracy: {test_acc:.4f}")
    print(f"[{name}] TEST confusion matrix (rows = true, cols = pred; 0=notblocked,1=blocked):")
    print(test_cm)
    print(f"\n[{name}] TEST classification report:")
    print(test_report)

    config = {
        "name": name,
        "seed": seed,
        "max_epochs": max_epochs,
        "lr": lr,
        "batch_size": batch_size,
        "use_lane_crop": use_lane_crop,
        "augment": True,
        "backbone": "nvidia/segformer-b0-finetuned-ade-512-512",
        "best_epoch": best_epoch,
        "best_val_acc": float(best_val_acc),
        "test_accuracy": float(test_acc),
        "min_delta": float(min_delta),
        "patience": patience,
    }

    config_path = os.path.join(save_dir, f"{name}_config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    print(f"\n[{name}] Saved run config to {config_path}")

    history = {
        "train_losses": train_losses,
        "val_accs": val_accs,
    }

    return model, (test_acc, test_cm, test_report, history)

"""In this cell, I train the baseline segformers classifier on full-frame images w data augmentation and early stopping to get a clean model run:
- min_delta = 0.001 (val accuracy must improve by at least 0.001)
- patience = 3 (stop if no such improvement for 3 consecutive epochs)
"""

baseline_model, baseline_metrics = train_one_model(
    name="baseline_full_frame_aug_es",
    use_lane_crop=False,               # using full images
    max_epochs=15,
    lr=1e-4,
    batch_size=8,
    save_dir=SAVE_DIR,
    seed=seed,
    min_delta=1e-3,                    # require at least 0.001 val_acc improvement
    patience=3                         # stop after 3 epochs with no such improvement
)

baseline_modelC, baseline_metricsC = train_one_model(
    name="baseline_lanecropped_frame_aug_es",
    use_lane_crop=True,               # using full images
    max_epochs=15,
    lr=1e-4,
    batch_size=8,
    save_dir=SAVE_DIR,
    seed=seed,
    min_delta=1e-3,                    # require at least 0.001 val_acc improvement
    patience=3                         # stop after 3 epochs with no such improvement
)

"""During training, we see early stopping is triggered at epoch 7, meaning that validation accuracy stopped improving meaningfully after epoch 4. Epoch 4's validation accuracy = 0.9174

When I evaluate it on the held-out cam68 test set,
- Test accuracy: 0.3947  
- For notblocked (27 images):  
  - 4 correctly predicted as notblocked  
  - 23 incorrectly predicted as blocked  
  - → very low recall (0.148), but precision = 1.0 (when the model says “notblocked”, it’s always correct, just very rare)

- For blocked (11 images):  
  - 11 correctly predicted as blocked  
  - 0 missed  
  - → recall = 1.0, but precision is only 0.324 (many notblocked images are being flagged as blocked)

Overall, this operating point is extremely conservative in the sense of rarely missing a blocked lane (100% blocked recall), but it generates many false alarms on notblocked images, leading to low overall accuracy (~ 39.5%). This motivates the threshold-tuning step that follows.

# Tuned classifier
"""

# use_lane_crop=False bc cropped was worse
train_loader, val_loader, test_loader = make_loaders(
    use_lane_crop=False,
    batch_size=8
)

# collect p(blocked) and true labels (probabilities)
def get_probs_and_labels(model, loader):
    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for pixel_values, labels in loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            logits = model(pixel_values)               # (B, 2)
            probs = torch.softmax(logits, dim=1)       # (B, 2)
            p_blocked = probs[:, 1]                    # prob of class 1 = "blocked"

            all_probs.extend(p_blocked.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_probs), np.array(all_labels)

val_probs, val_labels   = get_probs_and_labels(baseline_model, val_loader)
test_probs, test_labels = get_probs_and_labels(baseline_model, test_loader)

print("Val shape:", val_probs.shape, val_labels.shape)
print("Test shape:", test_probs.shape, test_labels.shape)

# val_probs[i] = model’s probability that image i is blocked
# val_labels[i] = 0 (notblocked) or 1 (blocked)

def sweep_thresholds(probs, labels, num_steps=101):
    thresholds = np.linspace(0.0, 1.0, num_steps)
    rows = []

    for t in thresholds:
        preds = (probs >= t).astype(int)  # 1 = blocked, 0 = notblocked

        # metrics for both classes
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels,
            preds,
            labels=[0, 1],   # [notblocked, blocked]
            zero_division=0
        )

        # unpack
        prec_not, rec_not, f1_not = precision[0], recall[0], f1[0]
        prec_blk, rec_blk, f1_blk = precision[1], recall[1], f1[1]

        rows.append({
            "threshold": t,
            "prec_blocked": prec_blk,
            "rec_blocked": rec_blk,
            "f1_blocked": f1_blk,
            "prec_notblocked": prec_not,
            "rec_notblocked": rec_not,
            "f1_notblocked": f1_not,
        })

    return rows

val_sweep = sweep_thresholds(val_probs, val_labels, num_steps=101)
len(val_sweep), val_sweep[:5]
val_df = pd.DataFrame(val_sweep)
val_df.head()

"""Idea 1: pick threshold based on not wanting to miss blockages (high recall), but still trying to keep reasonable precision

"""

# filter to thresholds where recall for blocked is >= 0.9
candidates = val_df[val_df["rec_blocked"] >= 0.9].copy()

if len(candidates) == 0:
    print("No thresholds reach recall >= 0.9 for blocked on val set.")
    # fall back to just the best F1 overall
    best_row = val_df.iloc[val_df["f1_blocked"].idxmax()]
else:
    # among those, pick the one with highest f1_blocked
    best_row = candidates.iloc[candidates["f1_blocked"].idxmax()]

best_threshold = float(best_row["threshold"])
print("Chosen threshold (p_blocked >= t → blocked):", best_threshold)
print(best_row)

# use best_threshold on the test probabilities
def evaluate_with_threshold(probs, labels, threshold):
    preds = (probs >= threshold).astype(int)

    acc = (preds == labels).mean()
    cm = confusion_matrix(labels, preds, labels=[0, 1])
    report = classification_report(
        labels,
        preds,
        labels=[0, 1],
        target_names=["notblocked", "blocked"],
        digits=3
    )
    return acc, cm, report

test_acc_t, test_cm_t, test_report_t = evaluate_with_threshold(
    test_probs, test_labels, best_threshold
)

print(f"Test accuracy at threshold={best_threshold:.3f}: {test_acc_t:.4f}")
print("Confusion matrix (rows = true, cols = pred; 0=notblocked,1=blocked):")
print(test_cm_t)
print("\nClassification report:")
print(test_report_t)

"""Idea 2: bc the recall-only approach did worse than the baseline, going to try and change the threshold metric to balanced accuracy ≈ average of class recalls


"""

val_df["balanced_acc"] = 0.5 * (val_df["rec_blocked"] + val_df["rec_notblocked"])

best_row_bal = val_df.iloc[val_df["balanced_acc"].idxmax()]
t_bal = float(best_row_bal["threshold"])
print("Best balanced-accuracy threshold on val:", t_bal)
print(best_row_bal)

test_acc_bal, test_cm_bal, test_report_bal = evaluate_with_threshold(
    test_probs, test_labels, t_bal
)
print(f"\nTest accuracy at threshold={t_bal:.3f}: {test_acc_bal:.4f}")
print("Confusion matrix:")
print(test_cm_bal)
print("\nClassification report:")
print(test_report_bal)

# t=0.5 threshold for comparison
row_05 = val_df.iloc[(val_df["threshold"] - 0.5).abs().idxmin()]
print("Metrics near t=0.5 on val:")
print(row_05)

"""# Understand why model making mistakes at 0.75 threshold - step 3"""

# using best threshold t_bal from step 2
operating_threshold = float(t_bal)
print("Using best operating threshold (p_blocked):", operating_threshold)

test_ds = test_loader.dataset
print("Test set size:", len(test_ds))

label_id2name = {v: k for k, v in label_map.items()}

# run model on test set and categorize TP / TN / FP / FN
def analyze_test_set(model, dataset, threshold):
    model.eval()
    records = []

    with torch.no_grad():
        for i in range(len(dataset)):
            # dataset.samples should hold (filepath, class_name)
            img_path, label_name = dataset.samples[i]
            pixel_values, label_tensor = dataset[i]

            pixel_values = pixel_values.unsqueeze(0).to(device)  # (1,3,H,W)
            true_id = label_tensor.item()

            logits = model(pixel_values)                 # (1, 2)
            probs = torch.softmax(logits, dim=1)         # (1, 2)
            p_blocked = probs[0, 1].item()               # prob of class 1 = "blocked"

            pred_id = int(p_blocked >= threshold)        # 1 = blocked, 0 = notblocked

            # categorize
            if true_id == 1 and pred_id == 1:
                err_type = "TP"   # blocked correctly detected
            elif true_id == 0 and pred_id == 0:
                err_type = "TN"   # notblocked correctly detected
            elif true_id == 0 and pred_id == 1:
                err_type = "FP"   # predicted blocked, actually notblocked
            else:  # true_id == 1 and pred_id == 0
                err_type = "FN"   # missed blockage

            records.append({
                "idx": i,
                "path": str(img_path),
                "true_id": true_id,
                "true_name": label_id2name[true_id],
                "pred_id": pred_id,
                "pred_name": label_id2name[pred_id],
                "p_blocked": p_blocked,
                "type": err_type,
            })

    return records

records = analyze_test_set(baseline_model, test_ds, operating_threshold)

TP = [r for r in records if r["type"] == "TP"]
TN = [r for r in records if r["type"] == "TN"]
FP = [r for r in records if r["type"] == "FP"]
FN = [r for r in records if r["type"] == "FN"]

print("\nCounts at threshold", operating_threshold)
print("  TP:", len(TP), "TN:", len(TN), "FP:", len(FP), "FN:", len(FN))

# helpers for summarziing and visualziing misclassified
def print_examples(examples, title, max_n=20):
    print(title, f"(showing up to {max_n})")
    for i, r in enumerate(examples[:max_n]):
        print(f"{i+1:2d}. {r['path']}")
        print(f"    true: {r['true_name']:<10s}  "
              f"pred: {r['pred_name']:<10s}  "
              f"p_blocked={r['p_blocked']:.3f}")
    print()

print_examples(FP, "False Positives (predicted blocked, actually notblocked)", max_n=50)
print_examples(FN, "False Negatives (missed blocked)", max_n=50)

def show_examples(examples, title, n=6):
    n = min(n, len(examples))
    if n == 0:
        print(f"No examples for {title}")
        return

    cols = 3
    rows = (n + cols - 1) // cols

    plt.figure(figsize=(5*cols, 4*rows))
    for i in range(n):
        r = examples[i]
        img = Image.open(r["path"]).convert("RGB")

        plt.subplot(rows, cols, i+1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(
            f"true: {r['true_name']}\n"
            f"pred: {r['pred_name']}\n"
            f"p_blocked={r['p_blocked']:.2f}",
            fontsize=9
        )

    plt.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.show()

# show a few of each type of mistake
show_examples(FP, "False Positives (notblocked → blocked)", n=8)
show_examples(FN, "False Negatives (blocked → notblocked)", n=8)

"""# Takeaways

- Model is p good at catching blocked (9/11 --> recall = 0.82)
- We have a high amount of false alarms due to the fact that the lane is very slightly blocked by a truck, but this doesn't count as fully obstructed for a bike to not be able to pass through at all. This isn't bad in my opinion though, because it just further prioritizes biker saftey.
- Model assigns high blocked probabilities (p_blocked from 0.78–0.94), showing  it may have learned a strong association between visual conetxt cues and the blocked label in training even when the true label is notblocked.
- Only 2 legitimately blocked imgs are missed and both are borderline cases (v close to being predicted as blocked w probabilities 0.722 and 0.691).
"""

